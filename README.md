## transformer
* selfattention 계산 예시
tranformer/selfattention_example.ipynb



## 가짜연구소 스터디 결과물
### 가짜 연구소
https://www.notion.so/PseudoLab-c42db6652c1b45c3ba4bfe157c70cf09
### 스터디명 
[NLP101-3] 한국인이라면 한국어 자연어 처리합시다.

### 교재 참조 사이트  
https://ratsgo.github.io/embedding/

### 실습결과 노트북
#### 단어수준 임베딩
* Word2ec
* FastText
* 잠재의미분석 (LSA)
* GloVe
* Swivel
#### 문장수준 임베딩
* Doc2Vec
* 잠재 디리클레 할당 (LDA)
* ELMo
* 트랜스포머 네트워크
* BERT


# BERT QA
## 2022년 일요일은 AI 스터디 발표 자료 (2222.04.24)
### 참조 사이트
https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/tree/master/7.PRETRAIN_METHOD

#### BERT QA KorQuAD 1.0 EDA
KNOU_STUDY_HOML2_KorQuAD_EDA.ipynb  

#### BERT QA Train, Validation, Inference
KNOU_STUDY_HOML2_BERT_QA.ipynb
